{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHKGDzEjVYo9"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "9GHPx8SJbBP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "split = \"train\"\n",
        "data = load_dataset(f\"transformersbook/codeparrot-{split}\", split=split, streaming=True)\n"
      ],
      "metadata": {
        "id": "flKMtvNQVvbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def any_keyword_in_string(keywords, string):\n",
        "  for keyword in keywords:\n",
        "    if keyword in string:\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "nZaLkHtFYW6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters = [ \"sklearn\", \"pandas\", \"matplotlib\", \"seaborn\"]\n",
        "example_1 = \"import numpy as np\"\n",
        "example_2 = \"import pandas as pd\"\n",
        "\n",
        "print(\n",
        "    any_keyword_in_string( filters, example_1,), any_keyword_in_string(filters, example_2,)\n",
        ")"
      ],
      "metadata": {
        "id": "auujZxj3YZRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "\n",
        "def filter_streaming_dataset(dataset, filters):\n",
        "  filtered_dict=defaultdict(list)\n",
        "  total=0\n",
        "  for sample in tqdm(iter(dataset)):\n",
        "    total+=1\n",
        "    if any_keyword_in_string(filters, sample[\"content\"]):\n",
        "      for k,v in sample.items():\n",
        "        filtered_dict[k].append(v)\n",
        "  print(f\"{len(filtered_dict['content'])/total:.2%} of data after filtering.\")\n",
        "  return Dataset.from_dict(filtered_dict)"
      ],
      "metadata": {
        "id": "lADVbmMyZurD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters = [ \"sklearn\", \"pandas\", \"matplotlib\", \"seaborn\"]\n",
        "# filtered_data = filter_streaming_dataset(data, filters)"
      ],
      "metadata": {
        "id": "EvZoPHcbdY9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "filtered dataset already in the hub "
      ],
      "metadata": {
        "id": "BPLUlIQReCFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "ds_train = load_dataset(\"huggingface-course/codeparrot-ds-train\", split=\"train\")\n",
        "ds_valid = load_dataset(\"huggingface-course/codeparrot-ds-valid\", split=\"validation\")\n",
        "\n",
        "raw_datasets = DatasetDict(\n",
        "    {\n",
        "        \"train\": ds_train.shuffle().select(range(50000)),\n",
        "        \"valid\": ds_valid.shuffle().select(range(500))\n",
        "    }\n",
        ")\n",
        "\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "gpPO3A5ndh4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "context_length=128\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"huggingface-course/code-search-net-tokenizer\")"
      ],
      "metadata": {
        "id": "27uiw9aZfFO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = tokenizer(raw_datasets[\"train\"][:2][\"content\"], truncation=True, max_length=context_length, return_overflowing_tokens=True, return_length=True)"
      ],
      "metadata": {
        "id": "3RJ-hwRDl2X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
        "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
        "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
      ],
      "metadata": {
        "id": "pcmxzz_ssObB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(elements):\n",
        "  outputs =tokenizer(elements[\"content\"], truncation=True, max_length=context_length, return_overflowing_tokens=True, return_length=True)\n",
        "  input_batch=[]\n",
        "  for len, output in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
        "    if len == context_length:\n",
        "      input_batch.append(output)\n",
        "  return {\"input_ids\" : input_batch}\n",
        "\n",
        "tokenized_dataset = raw_datasets.map(tokenize, batched=True, remove_columns= raw_datasets[\"train\"].column_names)\n",
        "\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "Ajrps1VLt7E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenized_dataset.save_to_disk(\"tokenized_dataset\")"
      ],
      "metadata": {
        "id": "zWDCVZrtVg3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_from_disk\n",
        "\n",
        "# tokenized_dataset = load_from_disk(\"tokenized_dataset\")\n",
        "# tokenized_dataset"
      ],
      "metadata": {
        "id": "VQy2Ey2mWfPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig, TFGPT2LMHeadModel\n",
        "\n",
        "config = AutoConfig.from_pretrained(\"gpt2\",\n",
        "                                    vocab_size=len(tokenizer), \n",
        "                                    n_ctx=context_length, \n",
        "                                    bos_token_id=tokenizer.bos_token_id,\n",
        "                                    eos_token_id=tokenizer.eos_token_id,)\n",
        "\n",
        "model = TFGPT2LMHeadModel(config)"
      ],
      "metadata": {
        "id": "TqtJCJWyxaMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model(model.dummy_inputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "HQuivZ7kNx08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import  DataCollatorForLanguageModeling\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False, return_tensors=\"tf\")"
      ],
      "metadata": {
        "id": "s1iVM7Q1O5Vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_train_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"train\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=True,\n",
        "    batch_size=32,\n",
        ")\n",
        "tf_eval_dataset = model.prepare_tf_dataset(\n",
        "    tokenized_dataset[\"valid\"],\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False,\n",
        "    batch_size=32,\n",
        ")"
      ],
      "metadata": {
        "id": "w7YjfR0HQzkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "_Zn-gNHcR6R_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import create_optimizer\n",
        "import tensorflow as tf\n",
        "\n",
        "num_train_steps = len(tf_train_dataset)\n",
        "optimizer, schedule = create_optimizer(\n",
        "    init_lr=5e-5,\n",
        "    num_warmup_steps=100,\n",
        "    num_train_steps=num_train_steps,\n",
        "    weight_decay_rate=0.01,\n",
        ")\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "# Train in mixed-precision float16\n",
        "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-htKgBnTCLF",
        "outputId": "fb15d504-3450-4fb6-d4e3-485164011bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import PushToHubCallback\n",
        "\n",
        "callback = PushToHubCallback(output_dir=\"codeparrot-ds\", tokenizer=tokenizer)\n",
        "\n",
        "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, callbacks=[callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orHtRKPvUw2v",
        "outputId": "34aa2baa-4e49-4cce-83e1-674b28c79377"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/Thabet/codeparrot-ds into local empty directory.\n",
            "WARNING:huggingface_hub.repository:Cloning https://huggingface.co/Thabet/codeparrot-ds into local empty directory.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 1669/42582 [>.............................] - ETA: 13:31:25 - loss: 5.4548"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7LhffVKMbLZz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}